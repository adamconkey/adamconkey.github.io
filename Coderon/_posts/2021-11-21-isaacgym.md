---
layout: post
title: Robot Behavior Data Collection in NVIDIA IsaacGym
description: Infrastructure for mass data collection in simulation to support robot behavior learning.
date: 2021-11-21
image: '/images/isaacgym/isaacgym_cover.png'
video_embed: 'https://www.youtube.com/embed/al1Gdr_zWyE?si=zswzaW4STZRwtOzL'
tags: [Robot Learning]
tags_color: '#4287f5'
featured: true
---

The last project I worked on during my Ph.D., [Planning under Uncertainty to Goal Distributions]({% post_url 2022-04-29-goal-distributions %}), involved learning robot skills from demonstration data. The other projects I had worked on involved learning from human demonstration data, but that was not an option for this project -- we needed a HUGE amount of data. Instead of having a human provide thousands and thousands of demonstrations, we wanted a way we could record data of the robot performing the tasks in an unsupervised way.

Our solution was to engineer some behaviors using simple task specifications  and motion-planned trajectories to execute them. This could have been done on the real robot, but that was also going to involve a fair amount of babysitting by a human. The solution was to record the data in simulation. But collecting data from just one simulator was going to take a really long time. The even better solution was to run many simulations at once! And fortunately, I was working on this at the advent of NVIDIA's [Isaac Gym](https://developer.nvidia.com/isaac-gym), which enabled running hundreds of simulations at once on a desktop computer (note Isaac Gym has since been deprecated, in favor of NVIDIA's Omniverse stack).

I developed a framework for mass data collection of robot skill executions in Isaac Gym. This was a huge undertaking which I spent much of the COVID pandemic lockdown toiling away on. It required thoughtful system engineering to develop the infrastructure for skill definition and execution, configuration of the simulation environments, recording and managing multimodal datasets, and running it all autonomously for days at a time.

---

## Software

| Repository | Description |
|------------|-------------|
| [ll4ma_isaac](https://bitbucket.org/robot-learning/ll4ma_isaac/src/main/) | Core repository for robot behavior data collection in Isaac Gym. |
| [ll4ma_moveit](https://bitbucket.org/robot-learning/ll4ma_moveit) | Interface to simplify requesting motion plans from [MoveIt](https://moveit.ai). |
| [ll4ma_util](https://bitbucket.org/robot-learning/ll4ma_util/src/main/) | Suite of utilities for all sorts of things including file handling, ROS utilities, data processing, CLI helpers, etc. |

--- 

## Behaviors

Behaviors are hierarchical in the sense that a behavior, e.g. pick up an object and place it, can be composed of more simple behaviors like a "pick up object" behavior and a "place object" behavior. Even an object-picking behavior is hierarchical with constituent steps of moving through free space near the object, grasping the object, and lifting it. The [Behavior](https://bitbucket.org/robot-learning/ll4ma_isaac/src/main/ll4ma_isaacgym/src/ll4ma_isaacgym/behaviors/behavior.py) class manages the hierarchical composition of behaviors and determines transitions between behaviors using the environment state as well as the state of the behaviors themselves. For example, if the robot gripper is sufficiently close to an object it knows it can now try to grasp it. And if a behavior fails, e.g. the object is dropped, it can halt the behavior or take some remedial action.

One simple base behavior is [MoveToPose](https://bitbucket.org/robot-learning/ll4ma_isaac/src/main/ll4ma_isaacgym/src/ll4ma_isaacgym/behaviors/move_to_pose.py), which enables the robot to move to a desired configuration in joint space or move its end-effector to a desired 6-DOF pose in Cartesian space. This behavior is a sub-behavior of nearly all other more complex behaviors. As long as you have some way of setting targets for the robot to move to you can get a lot done with this behavior. For example, in the [PickObject](https://bitbucket.org/robot-learning/ll4ma_isaac/src/main/ll4ma_isaacgym/src/ll4ma_isaacgym/behaviors/pick_object.py) behavior, target end-effector poses are set with respect to the object's coordinate frame, which in simulation can be read from the simulation state and in the real world can be acquired using a variety of object pose estimation techniques. Knowing the object's pose together with a target end-effector pose in the object frame lets you compute a target pose for the robot in the world frame, which the robot can then move to using the MoveToPose behavior.

To give you a sense of the hierarchy involved in a more complex behavior, consider [StackObjects](https://bitbucket.org/robot-learning/ll4ma_isaac/src/main/ll4ma_isaacgym/src/ll4ma_isaacgym/behaviors/stack_objects.py) which allows the robot to build up a stack of objects, i.e. build block towers. The behavior hierarchy looks like this:
```
StackObjects
  PickPlaceObject_1
    PickObject_1
      MoveToPose_approach
      CloseFingers
      MoveToPose_lift
    PlaceObject_1
      MoveToPose_above
      MoveToPose_place
      OpenFingers
    
    ...

  PickPlaceObject_N
```

It's a powerful feeling to be able to construct complex behaviors once you have the more basic component behaviors in your toolbox. In fact, the object-stacking behavior was not one I was using, one of my labmates wanted it for his experiments and I thought, well it's really just a sequence of multiple pick-place executions, I can make that behavior with what I already have! 

You might be wondering how hard it is to setup these behaviors. After all the legwork in the code, from the user perspective it's actually really easy! Just a simple YAML config:
```
# ============================================================================== #
#
# This config specifies a task for the iiwa+reflex to stack a tower of 3 blocks.
# If base_obj_position is specified in the behavior, it will first move the base
# block to that position then stack from there, otherwise it will keep the base
# block as it is and just stack the other block.
#
# ============================================================================== #

task:
  task_type: stack_objects
  behavior:
    name: stack_objects
    behaviors:
      - behavior_type: StackObjects
        name: stack_objects
        objects: [block_1, block_2, block_3]
        ignore_error: True
        allow_side_grasps: False
        remove_aligned_short_bb_face: True
        stack_buffer: 0.0
  extra_steps: 200
env: ~/isaac_ws/src/ll4ma_isaac/ll4ma_isaacgym/src/ll4ma_isaacgym/config/env_configs/iiwa_3stack.yaml
```
Some of those params are less intuitive at first glance, but they provide extra customization to your use case. For example, `allow_side_grasps: True` would expand the permissible grasp configurations of the robot to include grasping the object from the side instead of a top-down grasp. Disabling can be useful for building block towers, but if the tower's high enough you might _need_ to enable it so that you still get kinematically feasible trajectories for stacking the higher blocks.

I won't go into details on how all these params work, but the fact that you can specify 10 or so lines of YAML config and produce meaningful manipulations with a robot, that's pretty awesome!
