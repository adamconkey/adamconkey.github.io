---
layout: post
title: Robot Behavior Data Collection in NVIDIA IsaacGym
description: Infrastructure for mass data collection in simulation to support robot behavior learning.
date: 2021-11-21
image: '/images/isaacgym/isaacgym_cover.png'
video_embed: 'https://www.youtube.com/embed/al1Gdr_zWyE?si=zswzaW4STZRwtOzL'
tags: [Robot Learning]
tags_color: '#4287f5'
featured: true
---

The last project I worked on during my Ph.D., [Planning under Uncertainty to Goal Distributions]({% post_url 2022-04-29-goal-distributions %}), involved learning robot skills from demonstration data. The other projects I had worked on involved learning from human demonstration data, but that was not an option for this project -- we needed a HUGE amount of data. Instead of having a human provide thousands and thousands of demonstrations, we wanted a way we could record data of the robot performing the tasks in an unsupervised way.

Our solution was to engineer some behaviors using simple task specifications  and motion-planned trajectories to execute them. This could have been done on the real robot, but that was also going to involve a fair amount of babysitting by a human. The solution was to record the data in simulation. But collecting data from just one simulator was going to take a really long time. The even better solution was to run many simulations at once! And fortunately, I was working on this at the advent of NVIDIA's [Isaac Gym](https://developer.nvidia.com/isaac-gym), which enabled running hundreds of simulations at once on a desktop computer (note Isaac Gym has since been deprecated, in favor of NVIDIA's Omniverse stack).

I developed a framework for mass data collection of robot skill executions in Isaac Gym. This was a huge undertaking which I spent much of the COVID pandemic lockdown toiling away on. It required thoughtful system engineering to develop the infrastructure for skill definition and execution, configuration of the simulation environments, recording and managing multimodal datasets, and running it all autonomously for days at a time.

---

## Software

| Repository | Description |
|------------|-------------|
| [ll4ma_isaac](https://bitbucket.org/robot-learning/ll4ma_isaac/src/main/) | Core repository for robot behavior data collection in Isaac Gym. |
| [ll4ma_moveit](https://bitbucket.org/robot-learning/ll4ma_moveit) | Interface to simplify requesting motion plans from [MoveIt](https://moveit.ai). |
| [ll4ma_util](https://bitbucket.org/robot-learning/ll4ma_util/src/main/) | Suite of utilities for all sorts of things including file handling, ROS utilities, data processing, CLI helpers, etc. |

--- 

## Behaviors

Behaviors are hierarchical in the sense that a behavior, e.g. pick up an object and place it, can be composed of more simple behaviors like a "pick up object" behavior and a "place object" behavior. Even an object-picking behavior is hierarchical with constituent steps of moving through free space near the object, grasping the object, and lifting it. The [Behavior](https://bitbucket.org/robot-learning/ll4ma_isaac/src/main/ll4ma_isaacgym/src/ll4ma_isaacgym/behaviors/behavior.py) class manages the hierarchical composition of behaviors and determines transitions between behaviors using the environment state as well as the state of the behaviors themselves. For example, if the robot gripper is sufficiently close to an object it knows it can now try to grasp it. And if a behavior fails, e.g. the object is dropped, it can halt the behavior or take some remedial action.
