
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"I received my Ph.D. in Computing and Robotics from the University of Utah under the supervision of Dr. Tucker Hermans. My dissertation investigated skill planning under state and goal uncertainty for robot manipulation tasks.\nI have hands-on experience with a variety of robot platforms, sensors, and simulators. I am passionate about open source software development for robotics, particularly for the ROS framework and Linux.\nIn August 2022, I will be joining the Center for Autonomy Computing at HRL Laboratories as a Robotics Machine Learning Research Scientist.\n","date":1607817600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1607817600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I received my Ph.D. in Computing and Robotics from the University of Utah under the supervision of Dr. Tucker Hermans. My dissertation investigated skill planning under state and goal uncertainty for robot manipulation tasks.","tags":null,"title":"Adam Conkey","type":"authors"},{"authors":null,"categories":null,"content":"Lately I have been pursuing a research project requiring lots and lots of data for learning. In order to support this, I created a data collection framework for the NVIDIA IsaacGym simulator. I created a suite of hierarchical scripted behaviors that utilize MoveIt to generate motions, and simple heuristics to generate grasp and placement poses, e.g. selecting candidate poses from around the bounding box of the object.\nUsing the IsaacGym simulator, I’m able to run data collection for many environments at once (e.g. I can run 32 simulations at once and collect data for each of them). To support this, I also created a MoveIt interface package to make it easier to request plans for many different environments sequentially.\nThis work is still under heavy development as it is a component of my current research. I am working with other members in my lab to improve it and make it a more scalable option to collect interesting manipulation data to learn from.\nHere are some clips from my current setup:\n","date":1637452800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637452800,"objectID":"5ef1eef77f35b0fef1acc2facbeb276e","permalink":"https://adamconkey.github.io/project/isaacgym_data_collection/","publishdate":"2021-11-21T00:00:00Z","relpermalink":"/project/isaacgym_data_collection/","section":"project","summary":"Motion-planned manipulation behaviors using MoveIt and IsaacGym to collect data for learning algorithms.","tags":["MoveIt","IsaacGym"],"title":"IsaacGym Data Collection","type":"project"},{"authors":null,"categories":null,"content":"Our lab has developed some code to do pointcloud segmentation that finds points associated with an object of interest from a tabletop. The user must define a box region over which the segmentation algorithm will run. Previously, this was a tedious process of guessing and checking to see if you defined the region correctly, or required the user to break out a tape measure everytime the region was to be changed.\nI developed a tool for rviz using Markers and InteractiveMarkers that enables the user to set the segmentation region visually. The tool overlays a transparent cuboid on the pointcloud showing the region over which the segmentation algorithm will operate. The user can then change the bounds of the region by dragging the arrows attached to the object. Here is a video showing the user changing the region to segment either the left object or right object:\n","date":1637452800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637452800,"objectID":"d8e2b981a931b6e3d22ba8617eac1055","permalink":"https://adamconkey.github.io/project/pcd_segmentation/","publishdate":"2021-11-21T00:00:00Z","relpermalink":"/project/pcd_segmentation/","section":"project","summary":"Tool for rviz that enables user to select the region over which pointcloud segmentation will be performed.","tags":["Perception","rviz"],"title":"Region Selector for Pointcloud Segmentation","type":"project"},{"authors":null,"categories":null,"content":"I have developed a robot controller framework that uses my robot interface package to abstract away the particular requirements of each robot platform. As such, the controllers in this package should be useful for controlling any robot that has communication needs satisfied by one of the interfaces in the interface package. These controllers work for Gazebo simulations, and are written to inherit from an interface that mimics the Orocos RTT::TaskContext, with the intention that these controllers can be easily ported for real-time control on a physical robot (see our real-time controller package).\nThe current supported controllers are a joint space PD controller and a task space inverse dynamics controller. I have also implemented an operational space hybrid force/position controller and used it both in simulation and on a real Baxter robot. However, I have deprecated the force controller for now as the code design changed sufficiently from when I was using that controller, and am waiting until it is needed to update.\nHere is a video of the KUKA LBR4+ arm in Gazebo using a joint space controller: Here is a video of the ReFlex hand in Gazebo: ","date":1570233600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570233600,"objectID":"2d0eed6e10288a25c83379aaa589fdab","permalink":"https://adamconkey.github.io/project/robot_control/","publishdate":"2019-10-05T00:00:00Z","relpermalink":"/project/robot_control/","section":"project","summary":"Robot control framework for simulations in Gazebo. Designed to be extensible for easily adding new controllers. Works in conjunction with my robot interface package to decouple the controllers from any particular robot.","tags":["Robot Control","Simulation","Gazebo","KUKA","ReFlex","Allegro"],"title":"Robot Control in Gazebo","type":"project"},{"authors":null,"categories":null,"content":"Different robots have different communication requirements for control and diagnostics. In ROS, this can mean different message types, different topic names, different interfaces for a robot in simulation versus reality, and so on. This can greatly complicate things as you end up having to create separate controllers and monitoring nodes for each platform. I created a robot interface abstraction layer to try to mitigate these issues. The idea is that each robot has its own interface that implements the requirements particular to that platform, and each interface inherits from a common interface. I have implemented a number of controllers in my controller package that use the common interface, such that the controllers are agnostic to the robot being controlled, and they can be instantiated with any interface in this package.\nI have implemented interfaces for the KUKA LBR4+ (simulation), Baxter (real and simulation), and the Righthand Robotics ReFlex hand (simulation). Balakumar Sundaralingam has implemented an interface for the real and simulated Allegro hand.\n","date":1570233600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570233600,"objectID":"da65226899239fcd5a5db4ea58ec56d6","permalink":"https://adamconkey.github.io/project/robot_interface/","publishdate":"2019-10-05T00:00:00Z","relpermalink":"/project/robot_interface/","section":"project","summary":"Abstraction layer for controlling robots in a unified manner while accounting for robot platform-specific needs.","tags":["Robot Control","KUKA","ReFlex","Allegro"],"title":"Robot Interface","type":"project"},{"authors":null,"categories":null,"content":"We use the Orocos Real-Time Toolkit to control our KUKA LBR4+ arm, which ensures that our controllers meet the real-time requirements for the LBR4. The base implementation our lab started with comes from the lwr_hardware package from the Institut des Systèmes Intelligents et de Robotique (ISIR).\nI have added to this framework by writing a controller manager that allows safe swapping of controllers at runtime. I use this to rapidly record kinesthetic demonstrations on the robot by switching between a gravity compensation controller (for moving the arm kinesthetically), a high stiffness joint controller (for keeping the robot stationary), a joint PD controller (for moving the robot to a nominal starting position), and a task space inverse dynamics controller (for executing task space policies).\nThis video shows the various transitions between controllers in a learning from kinesthetic demonstration setting, where controller switching is initiated by button presses on an Xbox One controller: Here is a video of fully autonomous execution of a learned Probabilistic Movement Primitive policy using the joint PD controller: The LBR4+ comes with a Fast Research Interface (FRI), which allows us to run our custom controllers and read the robot state through ROS. Something I didn’t like was having to work out bugs in my controllers and controller manager on the real robot. So, I created a simulated FRI so that I could run our Orocos controllers in Gazebo before going live on the real robot. The simulation FRI hooks into the Gazebo simulation directly to read the robot state, and accepts the torque command output from our controllers to interface with the ROS control hardware interfaces.\n","date":1570233600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570233600,"objectID":"9858c28c7a5aa355c86b5853281ccf2f","permalink":"https://adamconkey.github.io/project/rt_robot_control/","publishdate":"2019-10-05T00:00:00Z","relpermalink":"/project/rt_robot_control/","section":"project","summary":"Real-time Orocos controllers for the KUKA LBR4+ robot. Includes a controller switching framework for safely swapping controllers at runtime, and a simulated FRI component that allows Orocos components to be tested Gazebo.","tags":["Robot Control","KUKA"],"title":"Real-Time Robot Control","type":"project"},{"authors":null,"categories":null,"content":"I’ve done some work to get the ReFlex TakkTile hand working in Gazebo. The URDF provided by RightHand robotics was intended only for visualization in rviz. I added inertial and collision models, joint hardware interfaces, Gazebo materials for mesh rendering, and replaced the parallel kinematic structure in the fingers with a serial structure. I elaborate a bit on this process on this Github issue. These changes can be found here and allow basic actuation of the ReFlex hand: I added contact sensors to the fingers to mimic the contact sensing offered by the real hand. Thank you to Mabel Zhang for her advice in setting this up! Here is a video of contacts being detected by the ReFlex hand in Gazebo using the contact sensor plugin (each green square is a separate contact sensor):\nI have also extended the basic contact sensing to also detect continuous pressure values. These are not yet fully functional yet though as they are not scaled correctly to the real hand. This is still an active area for development. I am also interested in eventually modeling the underactuated joints, and our lab is looking into the best way to handle this in Gazebo.\n","date":1570233600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570233600,"objectID":"85db125937805ff07f8b8f14a7743a51","permalink":"https://adamconkey.github.io/project/reflex_sensors/","publishdate":"2019-10-05T00:00:00Z","relpermalink":"/project/reflex_sensors/","section":"project","summary":"An actuated version of the ReFlex TakkTile hand and Gazebo plugins for simulating contact and pressure sensors on the fingers.","tags":["Simulation","Sensors","Gazebo","ReFlex"],"title":"ReFlex TakkTile in Gazebo Simulation","type":"project"},{"authors":null,"categories":null,"content":"I have effectively made rviz into a simulator. This was originally intended to be a lightweight simulator to serve as a training ground for learning robust policies in a large-latency teleoperation setting. If there is large communication between the remote robot and the operator (e.g. the robot is on Mars and the operator is on Earth), then direct teleoperation is infeasible, since the operator will have to wait several minutes to find out what effect their actions had at the remote site. The idea of this simulator is to use sensor data (e.g. object trackers based on camera feeds) to render a virtual environment that mimics the remote environment. Then the operator can give many demonstrations in the virtual environment, and a robust policy can be learned based on the collected demonstrations which can then be executed on the real robot.\nI used rviz to render the environment visually, and DART to simulate physics on the environment. The robot motion is controlled with a haptic input device, in my case a Phantom Omni (re-branded as the Sensable Geomagic Touch). The relative change in pose of the haptic device stylus is interpreted as a relative change in pose for the robot. Taking the pseudoinverse of the manipulator Jacobian and applying it to this change in pose results in a joint position update that is rendered in the simulator. Interaction forces in the environment are computed using DART, and these are rendered back to the user on the haptic device. The end result is the user can move the robot around, and when the robot makes contact with objects in the scene, the user feels the force as if they actually contacted the object in the real world with the haptic device stylus! The user also sees the force vector rendered in rviz.\nI created a simple rviz plugin that allows the user to enable/disable the robot motion and haptic feedback. It also provides the capability to reset the environment to its nominal state so that multiple demonstrations can be given on the same environment configuration. Here is a video of a simple interaction showing the features: I have some older iterations that used the Baxter robot, and tKinter for the user GUI:\nI have also used the previous iteration in Gazebo (without being mediated through any kinematic simulator). I had augmented the GUI to display the forces being applied in each dimension as registered by a simulated force sensor (using Gazebo’s force sensor plugin):\n","date":1570233600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570233600,"objectID":"917a62248360f6b493d75d7850d1c6eb","permalink":"https://adamconkey.github.io/project/teleop/","publishdate":"2019-10-05T00:00:00Z","relpermalink":"/project/teleop/","section":"project","summary":"A lightweight simulator with force rendering created with rviz and DART. Provides a training ground for learning robust policies from demonstration with a haptic input device.","tags":["Learning from Demonstration","rviz","DART","Simulation","Haptics"],"title":"Simulator for Teleoperation","type":"project"},{"authors":null,"categories":null,"content":"A demonstration recording framework for the Baxter robot to make it more efficient to collect kinesthetic demonstrations in a learning from demonstration setting. Everything is controlled from the robot using the button and display interfaces, allowing the teacher to rapidly give demonstrations without having to go back and forth between the robot and the computer. Current features include:\nStarting/stopping recording of robot and sensor data (e.g. joint states, end-effector pose, force sensor readings, etc.) using my logging framework. Moving arm back to a nominal starting position. Changing the nominal starting position. Zeroing out a mounted force/torque wrist sensor. Showing current status on the head display, and swiveling the head display to the left or right automatically depending on which arm is in use. Here is a short video showing off the features:\nHere is a video of it in action for recording a demonstration in an experiment: ","date":1570233600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570233600,"objectID":"c0e5c631d7167fddd9f7293e72012fc9","permalink":"https://adamconkey.github.io/project/baxter_record/","publishdate":"2019-10-05T00:00:00Z","relpermalink":"/project/baxter_record/","section":"project","summary":"A demonstration recorder for the Baxter robot utilizing the button and display interfaces to make it easier to record demonstrations when operating Baxter in gravity compensation mode.","tags":["Baxter","Learning from Demonstration","Human-Robot Interaction"],"title":"Baxter Demo Recorder","type":"project"},{"authors":null,"categories":null,"content":"This package offers customizable traces for task space trajectories in rViz. It uses Marker display types to show the end-effector position (or any other task space position) as it traverses through space. It’s configurable in that you can choose the color and line style of the trace (solid or dotted). It also allows you to visualize many trajectories at once, e.g. if you have samples from a distribution of trajectories. I have also implemented visualization of a full pose so that a Cartesian frame shows the pose over time, and also a static trace of poses so the entire pose trajectory can be seen at once.\nThis package additionally has some action servers/clients (joint and task space) for commanding trajectories. These can be used to track the status of a trajectory being executed and publish interpolated waypoints to a low-level controller.\nHere is a video of a couple different visualization styles: Here is a visualization of samples from a ProMP policy and one of the samples being executed with a mesh overlay of the robot: ","date":1570233600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570233600,"objectID":"dd3eb9938b68172f1e188f885da3f080","permalink":"https://adamconkey.github.io/project/trajectory_viz/","publishdate":"2019-10-05T00:00:00Z","relpermalink":"/project/trajectory_viz/","section":"project","summary":"Package for generating trajectory visualizations in rViz, including dynamic real-time visualizations showing a customizable trace of the robot's end-effector.","tags":["Visualization","rviz"],"title":"rviz Trajectory Visualization","type":"project"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Wowchemy’s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://adamconkey.github.io/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Adam Conkey","吳恩達"],"categories":["Demo","教程"],"content":"Overview The Wowchemy website builder for Hugo, along with its starter templates, is designed for professional creators, educators, and teams/organizations - although it can be used to create any kind of site The template can be modified and customised to suit your needs. It’s a good platform for anyone looking to take control of their data and online identity whilst having the convenience to start off with a no-code solution (write in Markdown and customize with YAML parameters) and having flexibility to later add even deeper personalization with HTML and CSS You can work with all your favourite tools and apps with hundreds of plugins and integrations to speed up your workflows, interact with your readers, and much more Get Started 👉 Create a new site 📚 Personalize your site 💬 Chat with the Wowchemy community or Hugo community 🐦 Twitter: @wowchemy @GeorgeCushen #MadeWithWowchemy 💡 Request a feature or report a bug for Wowchemy ⬆️ Updating Wowchemy? View the Update Tutorial and Release Notes Crowd-funded open-source software To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.\n❤️ Click here to become a sponsor and help support Wowchemy’s future ❤️ As a token of appreciation for sponsoring, you can unlock these awesome rewards and extra features 🦄✨\nEcosystem Hugo Academic CLI: Automatically import publications from BibTeX Inspiration Check out the latest demo of what you’ll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\nFeatures Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 34+ language packs including English, 中文, and Português Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files. Themes Wowchemy and its templates come with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\nChoose a stunning theme and font for your site. Themes are fully customizable.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1607817600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607817600,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"https://adamconkey.github.io/post/getting-started/","publishdate":"2020-12-13T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Welcome 👋 We know that first impressions are important, so we've populated your new site with some initial content to help you get familiar with everything in no time.","tags":["Academic","开源"],"title":"Welcome to Wowchemy, the website builder for Hugo","type":"post"},{"authors":["Adam Conkey","Tucker Hermans"],"categories":null,"content":" ","date":1604102400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604102400,"objectID":"dd8473afd7700ebcd1ed1e40ec07aea9","permalink":"https://adamconkey.github.io/publication/kl_planning/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/kl_planning/","section":"publication","summary":"We present a probabilistic planning framework for planning under uncertainty to goal distributions.","tags":["Planning","Goal Distributions","KL Divergence"],"title":"Planning under Uncertainty to Goal Distributions","type":"publication"},{"authors":["Adam Conkey"],"categories":null,"content":" ","date":1594425600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594425600,"objectID":"a31db7444eff45c82a92cedd9e4122ea","permalink":"https://adamconkey.github.io/publication/pioneers/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/pioneers/","section":"publication","summary":"I outline a long-term research project for learning robot behaviors in a lifelong learning setting leveraging multisensory observations.","tags":["Representation Learning","Deep Learning","Multisensory Perception"],"title":"Representation Learning for Multisensory Perception and Planning","type":"publication"},{"authors":["Adam Conkey","Tucker Hermans"],"categories":null,"content":" ","date":1571097600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571097600,"objectID":"f9be9d73549622796c53e08ce69e9da3","permalink":"https://adamconkey.github.io/publication/promp_active_learning/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/promp_active_learning/","section":"publication","summary":"An active learning approach to learning a library of Probabilistic Movement Primitives capable of generalizing over a bounded space.","tags":["Movement Primitives","Active Learning","Learning from Demonstration"],"title":"Active Learning of Probabilistic Movement Primitives","type":"publication"},{"authors":["Adam Conkey","Tucker Hermans"],"categories":null,"content":" ","date":1571097600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571097600,"objectID":"158346e0fdf91e156c76f216f1112889","permalink":"https://adamconkey.github.io/publication/dmp_hybrid_control/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/dmp_hybrid_control/","section":"publication","summary":"We learn a dynamic constraint frame from demonstration for hybrid force/position control using Cartesian Dynamic Movement Primitives.","tags":["Movement Primitives","Learning from Demonstration","Force Conrol"],"title":"Learning Task Constraints from Demonstration for Hybrid Force/Position Control","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://adamconkey.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"}]